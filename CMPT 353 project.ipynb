{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7710fd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import re\n",
    "import nltk\n",
    "#import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud, STOPWORDS #get_ipython().system('pip install wordcloud')\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4edb3f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_input = pd.read_csv(\"covid19_tweets.csv\")\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89977af",
   "metadata": {},
   "source": [
    "### Filter tweet by hashtag count>100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bfe81c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet= tweet_input['text']\n",
    "def convert(lst):\n",
    "    return ([i for item in lst for i in item.split()])\n",
    "panda_df = pd.DataFrame(data =tweet_input, columns = [\"text\",\"hashtags\"])\n",
    "panda_df=panda_df[panda_df['hashtags'].notnull()]\n",
    "df = pd.DataFrame(convert(panda_df['hashtags']))\n",
    "df=df.replace(',','', regex=True)\n",
    "df=df.replace(\"]\",'', regex=True)\n",
    "df=df.replace('\\'','', regex=True)\n",
    "df=df.replace('\\[''', '', regex=True)\n",
    "df.columns=['Words']\n",
    "s=df.value_counts()\n",
    "Words=pd.DataFrame(s.nlargest(155))\n",
    "modified = Words.reset_index()\n",
    "modified.columns=['Words',\"Counts\"]\n",
    "#print(modified) ## Mention count above 100\n",
    "selection = modified['Words'].tolist()\n",
    "filtered_tweet=tweet_input[tweet_input['hashtags'].notnull()]\n",
    "## Hashtags which contains these top 100 words\n",
    "mask = filtered_tweet.hashtags.apply(lambda x: any(item for item in selection if item in x))\n",
    "filtered_tweet=filtered_tweet[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "12693f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Time4fisticuffs</td>\n",
       "      <td>2020-07-25 12:27:14</td>\n",
       "      <td>@diane3443 @wdunlap @realDonaldTrump Trump nev...</td>\n",
       "      <td>['COVID19']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ethel mertz</td>\n",
       "      <td>2020-07-25 12:27:10</td>\n",
       "      <td>@brookbanktv The one gift #COVID19 has give me...</td>\n",
       "      <td>['COVID19']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DIPR-J&amp;K</td>\n",
       "      <td>2020-07-25 12:27:08</td>\n",
       "      <td>25 July : Media Bulletin on Novel #CoronaVirus...</td>\n",
       "      <td>['CoronaVirusUpdates', 'COVID19']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>🎹 Franz Schubert</td>\n",
       "      <td>2020-07-25 12:27:06</td>\n",
       "      <td>#coronavirus #covid19 deaths continue to rise....</td>\n",
       "      <td>['coronavirus', 'covid19']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hr bartender</td>\n",
       "      <td>2020-07-25 12:27:03</td>\n",
       "      <td>How #COVID19 Will Change Work in General (and ...</td>\n",
       "      <td>['COVID19', 'Recruiting']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179101</th>\n",
       "      <td>New Jersey Herald</td>\n",
       "      <td>2020-08-29 19:44:27</td>\n",
       "      <td>Wallkill school nurse adds COVID-19 monitoring...</td>\n",
       "      <td>['nurses', 'COVID19', 'coronavirus', 'schools']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179102</th>\n",
       "      <td>Pris</td>\n",
       "      <td>2020-08-29 19:44:23</td>\n",
       "      <td>we have reached 25mil cases of #covid19, world...</td>\n",
       "      <td>['covid19']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179103</th>\n",
       "      <td>AJIMATI AbdulRahman O.</td>\n",
       "      <td>2020-08-29 19:44:21</td>\n",
       "      <td>Thanks @IamOhmai for nominating me for the @WH...</td>\n",
       "      <td>['WearAMask']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179104</th>\n",
       "      <td>Jason</td>\n",
       "      <td>2020-08-29 19:44:16</td>\n",
       "      <td>2020! The year of insanity! Lol! #COVID19 http...</td>\n",
       "      <td>['COVID19']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179106</th>\n",
       "      <td>Gary DelPonte</td>\n",
       "      <td>2020-08-29 19:44:14</td>\n",
       "      <td>More than 1,200 students test positive for #CO...</td>\n",
       "      <td>['COVID19']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112737 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     user_name                 date  \\\n",
       "2              Time4fisticuffs  2020-07-25 12:27:14   \n",
       "3                  ethel mertz  2020-07-25 12:27:10   \n",
       "4                     DIPR-J&K  2020-07-25 12:27:08   \n",
       "5             🎹 Franz Schubert  2020-07-25 12:27:06   \n",
       "6                 hr bartender  2020-07-25 12:27:03   \n",
       "...                        ...                  ...   \n",
       "179101       New Jersey Herald  2020-08-29 19:44:27   \n",
       "179102                    Pris  2020-08-29 19:44:23   \n",
       "179103  AJIMATI AbdulRahman O.  2020-08-29 19:44:21   \n",
       "179104                   Jason  2020-08-29 19:44:16   \n",
       "179106           Gary DelPonte  2020-08-29 19:44:14   \n",
       "\n",
       "                                                     text  \\\n",
       "2       @diane3443 @wdunlap @realDonaldTrump Trump nev...   \n",
       "3       @brookbanktv The one gift #COVID19 has give me...   \n",
       "4       25 July : Media Bulletin on Novel #CoronaVirus...   \n",
       "5       #coronavirus #covid19 deaths continue to rise....   \n",
       "6       How #COVID19 Will Change Work in General (and ...   \n",
       "...                                                   ...   \n",
       "179101  Wallkill school nurse adds COVID-19 monitoring...   \n",
       "179102  we have reached 25mil cases of #covid19, world...   \n",
       "179103  Thanks @IamOhmai for nominating me for the @WH...   \n",
       "179104  2020! The year of insanity! Lol! #COVID19 http...   \n",
       "179106  More than 1,200 students test positive for #CO...   \n",
       "\n",
       "                                               hashtags  \n",
       "2                                           ['COVID19']  \n",
       "3                                           ['COVID19']  \n",
       "4                     ['CoronaVirusUpdates', 'COVID19']  \n",
       "5                            ['coronavirus', 'covid19']  \n",
       "6                             ['COVID19', 'Recruiting']  \n",
       "...                                                 ...  \n",
       "179101  ['nurses', 'COVID19', 'coronavirus', 'schools']  \n",
       "179102                                      ['covid19']  \n",
       "179103                                    ['WearAMask']  \n",
       "179104                                      ['COVID19']  \n",
       "179106                                      ['COVID19']  \n",
       "\n",
       "[112737 rows x 4 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = filtered_tweet[['user_name','date','text','hashtags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2ce102f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re      # Import REGEX\n",
    "def remove_mentions(text):\n",
    "    text = re.sub(\"@[A-Za-z0-9_]+\",\"\", text)\n",
    "    return text\n",
    "\n",
    "data['text'] =data.apply(lambda row: remove_mentions(row['text']), axis=1)\n",
    "import re, string, unicodedata \n",
    "def remove_non_ascii(words):\n",
    "    words = unicodedata.normalize('NFKD', words).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return words\n",
    "\n",
    "data['text'] =data.apply(lambda row: remove_non_ascii(row['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5fa6363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # prevent showing future warning\n",
    "tweet= data.loc[:,'text']\n",
    "##remove website\n",
    "tweet= tweet.str.replace(r\"http\\S+\", \"\")\n",
    "#remove hashtags\n",
    "tweet = tweet.str.replace(r'#','', regex=True)\n",
    "##remove punctuation\n",
    "tweet = tweet.str.replace('[^\\w\\s]','')\n",
    "##remove numbers\n",
    "tweet = tweet.str.replace(r'\\d+', '')\n",
    "##remove underscore\n",
    "tweet = tweet.str.replace(r\"\\W+_\\W+\", '', regex=True)\n",
    "tweet = tweet.str.replace(r\"_\", '', regex=True)\n",
    "##remove brackets\n",
    "tweet = tweet.str.replace('\\[.*?\\]', '')\n",
    "##convert to lower case\n",
    "tweet= tweet.str.lower()\n",
    "##drop empty rows\n",
    "tweet.dropna(inplace=True)\n",
    "##remove leading and ending white space\n",
    "tweet = tweet.str.strip()\n",
    "tweet = tweet.str.replace(r\"\\n\", '', regex=True)\n",
    "tweet = tweet.str.replace('  ', ' ', regex=True)\n",
    "data.loc[:,'text'] =tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cc3483",
   "metadata": {},
   "source": [
    "### Sentiment Analyze using NLTK(nlp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ac790752",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ref NLTK sentiment analyse https://www.nltk.org/howto/sentiment.html\n",
    "#get_ipython().system('pip install vaderSentiment')\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "scores=[]\n",
    "sentiment_=[]\n",
    "sentiment_fit_=[]\n",
    "for i in data['text']:\n",
    "    sentiment_score = sentiment_analyzer.polarity_scores(i)\n",
    "    score = sentiment_score['compound']\n",
    "    scores.append(score)\n",
    "data=data.assign(sentiment_score = scores)\n",
    "\n",
    "for i in data['sentiment_score']:\n",
    "    if i>0:\n",
    "        sentiment='positive'\n",
    "        sentiment_fit=1\n",
    "    elif i==0:\n",
    "        sentiment='neutral'\n",
    "        sentiment_fit=0\n",
    "    else:\n",
    "        sentiment='negative'\n",
    "        sentiment_fit=-1\n",
    "    sentiment_.append(sentiment)\n",
    "    sentiment_fit_.append(sentiment_fit)\n",
    "data=data.assign(sentiments=sentiment_)\n",
    "data=data.assign(sentiment_fit=sentiment_fit_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822c309c",
   "metadata": {},
   "source": [
    "### WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c051b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wordcloud=data[['text','hashtags']]\n",
    "word_cloud = WordCloud(background_color='black',colormap='Set2',stopwords=set(STOPWORDS),max_words=100,width = 3000, height = 2000,random_state=1,collocations=False).generate(str(df_wordcloud))\n",
    "fig = plt.figure(1, figsize=(10,10))\n",
    "plt.axis('off')\n",
    "fig.suptitle('Word Cloud for top 100 words', fontsize=20)\n",
    "fig.subplots_adjust(top=2.3)\n",
    "plt.imshow(word_cloud)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e26df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_agg=data[['date','sentiments','text']]\n",
    "data_agg['date'] =  pd.to_datetime(data_agg['date']).dt.date \n",
    "groups=data_agg.groupby(['date','sentiments'])\n",
    "data_agg =groups.aggregate('count').reset_index()\n",
    "data_agg=data_agg.rename(columns={'text': 'tweet_count'})\n",
    "\n",
    "sns.lineplot(data=data_agg, x=\"date\", y=\"tweet_count\", hue=\"sentiments\")\n",
    "plt.xticks(rotation = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6cd67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_agg.groupby('sentiments').sum() to get the count numbers\n",
    "data_sum=[30945,40239,40999]\n",
    "data_lable=['Negative', 'Netural', 'Positive']\n",
    "colors = sns.color_palette('bright')[7:10]\n",
    "plt.pie(data_sum,labels = data_lable, colors = colors, autopct='%.0f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5a1595",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "dd9519cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "select=data.iloc[0:10000] \n",
    "X=select['text']\n",
    "X = vectorizer.fit_transform(X).toarray()\n",
    "y=select['sentiment_fit']\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y,test_size = 0.4, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "859cc4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navie Bayes algorithm accuracy: 54.625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb=GaussianNB()\n",
    "nb.fit(X_train,y_train)\n",
    "#Test\n",
    "y_pred=nb.predict(X_valid)\n",
    "print(\"Navie Bayes algorithm accuracy:\",nb.score(X_valid,y_valid)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e0d6c65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knn algorithm accuracy= 42.35\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn=KNeighborsClassifier(n_neighbors=16)\n",
    "knn.fit(X_train,y_train)\n",
    "y_pred=knn.predict(X_valid)\n",
    "print(\"Knn algorithm accuracy=\",knn.score(X_valid,y_valid)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f06ab6a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10384/2475848301.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# yes, you can leave this loop in if you want.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mplot_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predictions-%i.png'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X, y)  \n",
    "\n",
    "    \n",
    "#knn\n",
    "knn_rgb_model= KNeighborsClassifier(n_neighbors = 12)\n",
    "knn_rgb_model.fit(X_train , y_train)\n",
    "\n",
    "\n",
    "#rf\n",
    "rf_rgb_model = RandomForestClassifier(n_estimators=100,\n",
    "     max_depth=3, min_samples_leaf=10)\n",
    "rf_rgb_model.fit(X_train, y_train)\n",
    "\n",
    "models = [bayes_rgb_model, knn_rgb_model, rf_rgb_model]\n",
    "for i, m in enumerate(models):  # yes, you can leave this loop in if you want.\n",
    "    m.fit(X_train, y_train)\n",
    "    plot_predictions(m)\n",
    "    plt.savefig('predictions-%i.png' % (i,))\n",
    "\n",
    "print(\n",
    "    bayes_rgb_model.score(X_valid, y_valid),\n",
    "    knn_rgb_model.score(X_valid, y_valid),\n",
    "    rf_rgb_model.score(X_valid, y_valid),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abb9c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['sentiment_score'].mean(), data['sentiment_score'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466aa1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use boxplot to check outlier\n",
    "import seaborn as sns\n",
    "sns.boxplot(x=data['sentiment_score']) \n",
    "Q1 = data['sentiment_score'].quantile(0.25)\n",
    "Q3 = data['sentiment_score'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "Lower_Fence = Q1 - (1.5 * IQR)\n",
    "#print(Lower_Fence)\n",
    "print(data.mean(), data.std())\n",
    "\n",
    "#remove outlier\n",
    "data=data.loc[data['sentiment_score']>= -0.8725]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1790ebc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['date'] =  pd.to_datetime(data['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def to_timestamp(t):\n",
    "    return datetime.timestamp(t)\n",
    "data['timestamp'] = data['date'].apply(to_timestamp)\n",
    "fit = stats.linregress(data['timestamp'] , data['sentiment_score'])\n",
    "data['prediction'] = data['timestamp']*fit.slope + fit.intercept\n",
    "print(\"fit.slope:\",fit.slope)\n",
    "#fit.slope<0, so the sentimen_score has a slight decrease during the period\n",
    "print(\"fit.intercept\",fit.intercept)\n",
    "print(\"fit.pvalue\", fit.pvalue) \n",
    "##since p value<0.05, reject H0, the ratings has been changing over time\n",
    "plt.xticks(rotation = 30)\n",
    "plt.rcParams.update({'figure.figsize':(10,8), 'figure.dpi':100})\n",
    "plt.scatter(data['date'] , data['sentiment_score'], c=data['sentiment_score'], cmap='Spectral')\n",
    "plt.colorbar()\n",
    "#plt.plot(data['date'] , data['sentiment_score'] , 'b.' , alpha = 0.5)\n",
    "plt.plot(data['date'] , data['timestamp']*fit.slope + fit.intercept , 'r-' , linewidth =3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2336ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals=data['sentiment_score']-data['prediction']\n",
    "plt.hist(residuals)#the requirement of normality can be softened with kind-of-normal data and n>=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fedc87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_matrix = np.corrcoef(positive_tweet['sentiment_score'], positive_tweet['prediction'])\n",
    "# corr = corr_matrix[0,1]\n",
    "# R_sq = corr**2\n",
    "# print(\"corr_matrix:\")\n",
    "# print(corr_matrix)\n",
    "# print(\"R-square:\",R_sq)\n",
    "# print(stats.linregress(data['timestamp'], data['sentiment_score']).rvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78d3efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97af6cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_raw = \"读入的数据\".split('\\n') # 按行分隔\n",
    "data_blue = []\n",
    "data_red = []\n",
    "word_count_blue = {}\n",
    "word_count_red = {}\n",
    "for line_data in data_raw:\n",
    "    word_list = line_data.split(\" \") # 分隔单词\n",
    "    if word_list[0] == \"BLUE\":\n",
    "        for i in range(1,len(word_list)):\n",
    "            if not is_excluded(word_list[i]): # 判断是否为停用词\n",
    "                data_blue.append(word_list[i])\n",
    "                # 统计单词出现次数\n",
    "                word_count_blue.setdefault(word_list[i], 0)\n",
    "                word_count_blue[word_list[i]] += 1\n",
    "    elif word_list[0] == \"RED\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0e07b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
